{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like', 'training', 'datascience', 'I', 'RRITEC', '.'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "scene_one = \"I like datascience. I like RRITEC training\"\n",
    "sentences = sent_tokenize(scene_one) #\n",
    "tokenized_sent = word_tokenize(sentences[1])\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5874e9b7fea6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# only digits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenize_digits_and_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\d+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_digits_and_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"he has 8 dogs and 11 cats\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# only alphabers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# only digits\n",
    "tokenize_digits_and_words=('\\d+')\n",
    "re.findall(tokenize_digits_and_words,\"he has 8 dogs and 11 cats\")\n",
    "\n",
    "# only alphabers\n",
    "tokenize_digits_and_words = ('[a-z]\\d+')\n",
    "re.findall(tokenize_digits_and_words,\"he has 8 dogs and 11 cats\")\n",
    "\n",
    "#Both digits and alphabets\n",
    "tokenize_digits_and_words=('[a-z]\\d+')\n",
    "re.findall(tokenize_digits_and_words,\"he has 8 dogs and 11 cats\")\n",
    "\n",
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "re.match('[a-z0-9]+',my_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " 'in',\n",
       " 'Mercea',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import regexp_tokenize\n",
    "my_string = \"SOLDIER#1 : Found them? in Mercea The coconut's tropical\"\n",
    "pattern1 = r\"(\\w+|#\\d|\\?!)\"\n",
    "regexp_tokenize(my_string,pattern=pattern1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#', 'python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@RRITC', ':', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Regex with NLTK tweet tokenization\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweets = ['This is the best #nlp exercise ive found online! # python',\n",
    "          '#NLP is super fun! <3 #learning',\n",
    "         'Thanks @RRITC:#nlp #python']\n",
    "#Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = \"r#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in tweet in the tweets list\n",
    "regexp_tokenize(tweets[0],pattern1)\n",
    "\n",
    "#Write a pattern that matches both @ and hashtags \n",
    "pattern2 = r\"([#|@]\\w+)\"\n",
    "\n",
    "#Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1],pattern2)\n",
    "\n",
    "#Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 4), ('the', 3), ('The', 2), ('is', 2), ('.', 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building a counter with bag-of-words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat like the box.The cat is over the cat.\"\"\"))\n",
    "counter\n",
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a counter with bag-of-words and identify the topic\n",
    "\n",
    "article = open('nlp_topic_identification.txt','r').read()\n",
    "#import Counter\n",
    "from collections import Counter\n",
    "\n",
    "#Tokenize the article:tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "#Convert the tokens in to lowercase:lower_tokens\n",
    "lower_tokens=[t.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
